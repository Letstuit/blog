{
  
    
        "post0": {
            "title": "Finding a standard dataset format for machine learning",
            "content": "With OpenML, we aim to take a worry-free, &#39;zen&#39;-like approach to working with machine learning datasets, making them easy and reliable to use. We want to offer training data that can be easily (or automatically) used. As such, you can now load any dataset and start building models without any manual intervention. For historical reasons, we have done this by internally storing all data in the ARFF data format, which is a simple single-table format, including meta-data such as the correct feature data types. . However, this format is currently holding us back: it is not ideal for storing large datasets, the format is only loosely defined causing different parsers to behave differentlyeDd, and many new machine learning tasks require multi-table data. For instance, image segmentation or object detection tasks have both images and varying amounts of annotations per image. . Hence, we started searching for a better data format that we can use to store (and share) machine learning datasets in the foreseeable future. This blog post presents out process and insights. We would love to hear your thoughts and experiences before we make any decision on how to move forward. . Scope . We first defined the general scope of the usage of the format: . We mainly need a format that is useful for data storage and transmission. We can always convert data during upload or download in OpenML&#39;s client APIs. For instance, people may upload a Python pandas dataframe to OpenML, and later get the same dataframe back, without realizing or caring how the data was stored in the meantime. If people want to store the data locally, they can download it in the format they like (e.g. a memory-mapped format like Arrow/Feather for fast reading or TFRecords for people using TensorFlow). Additional code can facilitate such conversions. | The format should allow storing most machine learning datasets, including images, video, audio, text, graphs, and multi-tabular data such as object recognition tasks and relational data. | To make the data easy to use, there should be a standard way to represent specific types of data (i.e. a fixed schema that can be verified), so that we can easily read and store datasets in a uniform way. For instance, all tabular data should be stored in a uniform way. | . Impact on OpenML (simplicity, maintenance) . Since OpenML is a community project, we want to keep it as easy as possible to use and maintain: . We prefer a single internal data format rather than multiple ones, since the latter would impose more maintenance on both server-side and client-side. | Even if we choose a flexible data format, this does not mean that we will store arbitrary dataset files for which we cannot guarantee data quality and easy of use. | We will require machine-readable schemas(in a specific language) that describe how a certain &#39;type&#39; data is formatted, to ensure ease of use and maintainability. Examples would be a schema for tabular data, a schema for annotated image data, etc. Every dataset should specify the schema it satisfied. We should be able to validate whether the dataset indeed satisfies the schema. Every OpenML task should be mapped to a schema, so that it is clear how the data must be used in training and testing. OpenML&#39;s clients can then use these schema to handle new types of data. | OpenML would have a set of allowed schemas. We will stick to tabular data until other schemas are defined, and we are sure that we can validate and handle those datasets. | When no agreed upon schema exists, we could offer a forum for the community to discuss and agree on a standard schema, in collaboration with other initiatives (e.g. frictionlessdata). [For instance, new schemas could be created in a github repo to allow people to do create pull requests. They could be effectively used once they are merged.] | . Requirements . To draw up a shortlist of data formats, we used the following requirements: . Maintenance. We are still a small community so we would prefer a format which is stable and fully maintained over something which we would have to maintain ourselves. | Support for the format in various programming languages, including well-maintained and stable libraries. | Support for reading the data without copying everything into memory (e.g. incremental reads/writes), and for subselecting parts of the data and operating only on that. | Ideally, there is a way to detect bitflip errors during storage or transmission. | The dataset format should support multiple &quot;resources&quot; to support cases when we would like to store collections of files or multiple relational tables. | Support for storing binary blobs and vectors of different lengths. | Anything else being equal, the dataset format should be compact. | Ideally, there should be support for storing sparse data. | A nice to have is that we can store some meta-data inside the file. OpenML can generate more extensive meta-data on the fly, but storing a minimal set inside the file may be useful. | . Shortlist . We decided to investigate the following formats in more detail: . Apache Arrow / Feather . Benefits: . Great for locally caching files after download | Memory-mapped, so very fast reads | . Drawbacks: . Not stable enough yet and not ideal for long-term storage. The authors also discourage it for long-term storage. | Limited to one data structure per file, but that data structure can be complex (e.g. dict). | . Parquet . Benefits: . Used in industry a lot, especially in combination with Apache Spark. Good community of practice | Well-supported and maintained, but not all Parquet features are supported in every library. E.g. the python library does not support partial read/writes. | Simple structure | Built-in compression (columnar storage), very efficient long-term data storage | . Drawbacks: . The different parsers (e.g. Parquet support inside Arrow, fastparquet) implement different parts of the Parquet format (and different set of compression algorithms), meaning that the output may not be compatiblewith other parsers (in other languages). | Support limited to single-table storage. There is good support to convert to and from pandas DataFrames, but there is less support for more complex data structures. Also, Parquet files created by other libraries might not be readable into pandas. More complicated data schemas might be possible, but are not supported in Python. For instance, there doesn&#39;t seem to be an apparent way to store an object detection dataset (with images and annotations) as a single parquet file. | Limited support for incremental reading/writing. None of the parsers we looked at (e.g. Parquet support inside Arrow, fastparquet) allows incremental writes, which may be an issue for large datasets when we (or end users) cannot load the data into memory. We can easily store large datasets in multiple (partitioned) parquet files, but that would make them less easy to use in OpenML. | . SQLite . Benefits: . SQLite was the easiest to use. It was comparably fast to HDF5 in our tests. | Very good support in all languages. For instance, it is built-in in Python. | Very flexible access to parts of the data. SQL queries can be used to select any subset of the data. | . Drawback: . It supports only 2000 columns, and we have quite a few datasets with more than 2000 features. Hence, storing large tabular data will require mapping data differently, which would add a lot of additional complexity. | Writing SQL queries requires knowledge of the internal data structure (tables, keys,…). | . HDF5 . Benefits: . Very good support in all languages. Has well-tested parsers, all using the same C implementation. | Widely accepted format in the deep learning community to store data and models. | Widely accepted format in many scientific domains (e.g. astronomy, bioinformatics,…) | Provides built-in compression. Constructing and loading datasets was reasonably fast. | Very flexible. Should allow to store any machine learning dataset as a single file. | Allows easy inclusion of meta-data inside the file, creating a self-contained file. | Self-descriptive: the structure of the data can be easily read programmatically. For instance, &#39;h5dump -H -A 0 mydata.hdf5&#39; will give you a lot of detail on the structure of the dataset. | . Drawbacks: . Complexity. We cannot make any a priori assumptions about how the data is structured. We need to define schema and implement code that automatically validates that a dataset follows a specific schema (e.g. e.g. using h5dump to see whether it holds a single dataframe that we could load into pandas). | The format has a very long and detailed specification. While parsers exist we don&#39;t really know whether they are fully compatible with each other. There may be unknown bugs. | . CSV . Benefits: . Very good support in all languages. | Easy to use, requires very little additional tooling | Easy versioning with git LFS. Changes in different versions can be observed with a simple git diff. | The current standard used in frictionlessdata. | There exist schema to express certain types of data in CSV (see frictionlessdata). | . Drawbacks: . Not very efficient for storing floating point numbers | Not ideal for very, very large datasets (when data does not fit in memory/disk) | Many different dialects exist. We need to decide on a standardized dialect and enforce that only that dialect is used on OpenML (https://frictionlessdata.io/specs/csv-dialect/). The dialect specified in RFC4180, which uses the comma as delimiter and the quotation mark as quote character, is often recommended. | . Overview .   Parquet HDF5 SQLite CSV . Consistency across different platforms | Check or clarify | ✅ | ✅ | ✅(dialect) | . Support and documentation | ✅ | ✅ | ✅ | ✅ | . Supports very large and high-dimensional datasets | ✅ | ✅ | ❌(limited nr. columns per table) | ✅ Storing tensors requires flattening. | . Simplicity | ✅ | ❌(basically full file system) | ✅(database) | ✅ | . Metadata support | Only minimal | ✅ | ✅ | ❌(requires separate metadata file) | . Maintenance | Apache project, open and quite active | Closed group, but active community on Jira and conferences | Run by a company. Community interaction via email list. | ✅ | . Available examples of usage in ML | ✅ | ✅ | ❌ | ✅ | . Allows incremental reads/writes | Yes, but not supported by current Python libs | ✅ | ✅ | Yes (but not random access) | . Flexibility | Only tabular data supported | Very flexible, maybe too flexible | Relational multi-table | Only tabular | . Performance benchmarks . There exist some prior benchmarks (here and here) on storing dataframes. These only consider single-table datasets. We also ran our own benchmark to compare the writing performance of those data formats for more complex machine learning datasets, such as pedestrian detection and music analysis (note that we could not find a way to store these in one file in Parquet). Reading benchmarks have yet to be done. . Version control . Version control for large datasets is tricky. For CSV, we could use git LFS store the datasets and have automated versioning of datasets. The binary formats do not allow us to track changes in the data, only to recover the exact versions of the datasets you want (and their metadata). . We found it quite easy to export all OpenML dataset to GitLab: https://gitlab.com/data/d/openml . We need your help! If we have missed any format we should investigate, or misunderstood those we have investigated, or missed some best practice, please tell us. . Email: openmlhq@googlegroups.com . React via github: https://github.com/openml/openml-data/issues/29 . Contributors to this blog post: . Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren, Pieter .",
            "url": "https://openml.github.io/blog/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "relUrl": "/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "date": " • Mar 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://openml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://openml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}