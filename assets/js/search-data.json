{
  
    
        "post0": {
            "title": "Finding a standard dataset format for machine learning",
            "content": "With OpenML, we aim to take a worry-free, &#39;zen&#39;-like approach to working with machine learning datasets, making them easy and reliable to use. We want to offer training data that can be easily (or automatically) used. As such, you can now load any dataset and start building models without any manual intervention. For historical reasons, we have done this by internally storing all data in the ARFF data format, which is a simple single-table format, including meta-data such as the correct feature data types. . However, this format is currently holding us back: it is not ideal for storing large datasets, the format is only loosely defined causing different parsers to behave differentlyeDd, and many new machine learning tasks require multi-table data. For instance, image segmentation or object detection tasks have both images and varying amounts of annotations per image. . Hence, we started searching for a better data format that we can use to store (and share) machine learning datasets in the foreseeable future. This blog post presents out process and insights. We would love to hear your thoughts and experiences before we make any decision on how to move forward. . Scope . We first defined the general scope of the usage of the format: . We mainly need a format that is useful for data storage and transmission. We can always convert data during upload or download in OpenML&#39;s client APIs. For instance, people may upload a Python pandas dataframe to OpenML, and later get the same dataframe back, without realizing or caring how the data was stored in the meantime. If people want to store the data locally, they can download it in the format they like (e.g. a memory-mapped format like Arrow/Feather for fast reading or TFRecords for people using TensorFlow). Additional code can facilitate such conversions. | The format should allow storing most machine learning datasets, including images, video, audio, text, graphs, and multi-tabular data such as object recognition tasks and relational data. | To make the data easy to use, there should be a standard way to represent specific types of data (i.e. a fixed schema that can be verified), so that we can easily read and store datasets in a uniform way. For instance, all tabular data should be stored in a uniform way. | . Impact on OpenML (simplicity, maintenance) . Since OpenML is a community project, we want to keep it as easy as possible to use and maintain: . We prefer a single internal data format rather than multiple ones, since the latter would impose more maintenance on both server-side and client-side. | Even if we choose a flexible data format, this does not mean that we will store arbitrary dataset files for which we cannot guarantee data quality and easy of use. | We will require machine-readable schemas(in a specific language) that describe how a certain &#39;type&#39; data is formatted, to ensure ease of use and maintainability. Examples would be a schema for tabular data, a schema for annotated image data, etc. Every dataset should specify the schema it satisfied. We should be able to validate whether the dataset indeed satisfies the schema. Every OpenML task should be mapped to a schema, so that it is clear how the data must be used in training and testing. OpenML&#39;s clients can then use these schema to handle new types of data. | OpenML would have a set of allowed schemas. We will stick to tabular data until other schemas are defined, and we are sure that we can validate and handle those datasets. | When no agreed upon schema exists, we could offer a forum for the community to discuss and agree on a standard schema, in collaboration with other initiatives (e.g. frictionlessdata). [For instance, new schemas could be created in a github repo to allow people to do create pull requests. They could be effectively used once they are merged.] | . Requirements . To draw up a shortlist of data formats, we used the following requirements: . Maintenance. We are still a small community so we would prefer a format which is stable and fully maintained over something which we would have to maintain ourselves. | Support for the format in various programming languages, including well-maintained and stable libraries. | Support for reading the data without copying everything into memory (e.g. incremental reads/writes), and for subselecting parts of the data and operating only on that. | Ideally, there is a way to detect bitflip errors during storage or transmission. | The dataset format should support multiple &quot;resources&quot; to support cases when we would like to store collections of files or multiple relational tables. | Support for storing binary blobs and vectors of different lengths. | Anything else being equal, the dataset format should be compact. | Ideally, there should be support for storing sparse data. | A nice to have is that we can store some meta-data inside the file. OpenML can generate more extensive meta-data on the fly, but storing a minimal set inside the file may be useful. | . Shortlist . We decided to investigate the following formats in more detail: . Apache Arrow / Feather . Benefits: . Great for locally caching files after download | Memory-mapped, so very fast reads | . Drawbacks: . Not stable enough yet and not ideal for long-term storage. The authors also discourage it for long-term storage. | Limited to one data structure per file, but that data structure can be complex (e.g. dict). | . Parquet . Benefits: . Used in industry a lot, especially in combination with Apache Spark. Good community of practice | Well-supported and maintained, but not all Parquet features are supported in every library. E.g. the python library does not support partial read/writes. | Simple structure | Built-in compression (columnar storage), very efficient long-term data storage | . Drawbacks: . The different parsers (e.g. Parquet support inside Arrow, fastparquet) implement different parts of the Parquet format (and different set of compression algorithms), meaning that the output may not be compatiblewith other parsers (in other languages). | Support limited to single-table storage. There is good support to convert to and from pandas DataFrames, but there is less support for more complex data structures. Also, Parquet files created by other libraries might not be readable into pandas. More complicated data schemas might be possible, but are not supported in Python. For instance, there doesn&#39;t seem to be an apparent way to store an object detection dataset (with images and annotations) as a single parquet file. | Limited support for incremental reading/writing. None of the parsers we looked at (e.g. Parquet support inside Arrow, fastparquet) allows incremental writes, which may be an issue for large datasets when we (or end users) cannot load the data into memory. We can easily store large datasets in multiple (partitioned) parquet files, but that would make them less easy to use in OpenML. | . SQLite . Benefits: . SQLite was the easiest to use. It was comparably fast to HDF5 in our tests. | Very good support in all languages. For instance, it is built-in in Python. | Very flexible access to parts of the data. SQL queries can be used to select any subset of the data. | . Drawback: . It supports only 2000 columns, and we have quite a few datasets with more than 2000 features. Hence, storing large tabular data will require mapping data differently, which would add a lot of additional complexity. | Writing SQL queries requires knowledge of the internal data structure (tables, keys,…). | . HDF5 . Benefits: . Very good support in all languages. Has well-tested parsers, all using the same C implementation. | Widely accepted format in the deep learning community to store data and models. | Widely accepted format in many scientific domains (e.g. astronomy, bioinformatics,…) | Provides built-in compression. Constructing and loading datasets was reasonably fast. | Very flexible. Should allow to store any machine learning dataset as a single file. | Allows easy inclusion of meta-data inside the file, creating a self-contained file. | Self-descriptive: the structure of the data can be easily read programmatically. For instance, &#39;h5dump -H -A 0 mydata.hdf5&#39; will give you a lot of detail on the structure of the dataset. | . Drawbacks: . Complexity. We cannot make any a priori assumptions about how the data is structured. We need to define schema and implement code that automatically validates that a dataset follows a specific schema (e.g. e.g. using h5dump to see whether it holds a single dataframe that we could load into pandas). | The format has a very long and detailed specification. While parsers exist we don&#39;t really know whether they are fully compatible with each other. There may be unknown bugs. | . CSV . Benefits: . Very good support in all languages. | Easy to use, requires very little additional tooling | Easy versioning with git LFS. Changes in different versions can be observed with a simple git diff. | The current standard used in frictionlessdata. | There exist schema to express certain types of data in CSV (see frictionlessdata). | . Drawbacks: . Not very efficient for storing floating point numbers | Not ideal for very, very large datasets (when data does not fit in memory/disk) | Many different dialects exist. We need to decide on a standardized dialect and enforce that only that dialect is used on OpenML (https://frictionlessdata.io/specs/csv-dialect/). The dialect specified in RFC4180, which uses the comma as delimiter and the quotation mark as quote character, is often recommended. | . Overview .   Parquet HDF5 SQLite CSV . Consistency across different platforms | Check or clarify | ✅ | ✅ | ✅(dialect) | . Support and documentation | ✅ | ✅ | ✅ | ✅ | . Supports very large and high-dimensional datasets | ✅ | ✅ | ❌(limited nr. columns per table) | ✅ Storing tensors requires flattening. | . Simplicity | ✅ | ❌(basically full file system) | ✅(database) | ✅ | . Metadata support | Only minimal | ✅ | ✅ | ❌(requires separate metadata file) | . Maintenance | Apache project, open and quite active | Closed group, but active community on Jira and conferences | Run by a company. Community interaction via email list. | ✅ | . Available examples of usage in ML | ✅ | ✅ | ❌ | ✅ | . Allows incremental reads/writes | Yes, but not supported by current Python libs | ✅ | ✅ | Yes (but not random access) | . Flexibility | Only tabular data supported | Very flexible, maybe too flexible | Relational multi-table | Only tabular | . Performance benchmarks . There exist some prior benchmarks (here and here) on storing dataframes. These only consider single-table datasets. We also ran our own benchmark to compare the writing performance of those data formats for more complex machine learning datasets, such as pedestrian detection and music analysis (note that we could not find a way to store these in one file in Parquet). Reading benchmarks have yet to be done. . Version control . Version control for large datasets is tricky. For CSV, we could use git LFS store the datasets and have automated versioning of datasets. The binary formats do not allow us to track changes in the data, only to recover the exact versions of the datasets you want (and their metadata). . We found it quite easy to export all OpenML dataset to GitLab: https://gitlab.com/data/d/openml . We need your help! If we have missed any format we should investigate, or misunderstood those we have investigated, or missed some best practice, please tell us. . Email: openmlhq@googlegroups.com . Contributors to this blog post: . Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren, Pieter Gijsbers .",
            "url": "https://openml.github.io/blog/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "relUrl": "/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "OpenML workshop at Dagstuhl",
            "content": "October 2019 . Twice a year the OpenML community meets for a workshop/hackathon/unconference. We improve the platform, discuss and learn for 5 days. If that sounds interesting to you, get in touch or follow updates on meet.openml.org. . This time the workshop took place at Dagstuhl, a great place for Seminars. . . . We had several breakouts where workshop attendees can join to learn, discuss and progress OpenML. In the following we discuss some of the topics we touched. . Science projects . Brainstorm on scientific projects to do with OpenML. Prioritize impactful, well-defined research ideas. We came up with quite a long list of very promising research questions that should actually be quite easy to answer based on OpenML. Many of these are along the lines of empirically providing evidence to verify or bust commonly-held beliefs in the community. Frank would gladly hire a strong postdoc (or quite independent PhD student) and maybe a research engineer, to work on these scientific questions under the umbrella of “evidence-based machine learning with OpenML”. . Benchmarking using OpenML . Define guidelines on how to define world-class benchmarks and how to run them. . Diverse datasets . New and more diverse datasets. . Dataset quality . How to measure data quality and how to improve the quality of datasets on OpenML. . OpenML use cases for novices . Shortlist common use cases and start writing accessible blog posts for novice users. If you are a OpenML newbie we need your help with this topic. . The output of this breakout will be at least one blog post. Keep an eye out for them here :) . Running a competition using OpenML . We assessed the current issues with running in-class competitions for teaching purposes using OpenML (biggest one: easy use for non-developers) and brainstormed on a new competition format with related competitions, one for each component of a solution, such as HPO, creating good meta-features, creating a good search space, etc. . Planning future workshops . Decide on location and timing for the next couple of workshops. The next OpenML workshop will be in Spring (week of March 30th or week of April 14th) close to Munich. For updates check http://meet.openml.org. The workshop will be cohosted with some other Open Source Machine Learning projects. . Furthermore a workshop in Austin is being planned for next summer and a datathon is in planning. We are planning to organize dev sprints at various PyCons next year. . New frontend . Feedback session on new frontend, Additional visualization for datasets . Future of client APIs . Currently, a lot of resources are bound developing different client APIs, such as the Python API, R API and Java API. We discussed how we can better share work and code between the different APIs and the server. For now we are working on automatically generating the Swagger API documentation from the PHP function documentation, which in turn will allow us to generate (documented) parts of the APIs, reduce the need for maintenance and will help to spread updates on the API faster. . Flow 2.0 design . Current flow design used in OpenML was inspired by Weka, but through time many limitations have been identified, primarily that existing flow does not allow duplicate use of same component and that it cannot express DAG-based ML programs. We started working on a new specification building on insights from mlr3 and d3m projects, centered around DAG representation. Current plan is to prepare a draft specification and implement prototype converters between other systems and this new specification. Once we do that we will re-evaluate the amount of work it took to build those prototypes and how well the specification satisfied those other systems. . Random Bot . The LRZ in Munich provided us with CPU time during the SuperMUC-NG supercomputer test phase, which we used to perform experiments of popular machine learning algorithms with random hyperparameter configurations. This resulted in millions of data points on more than a hundred datasets that we will analyse and publish. The data can be used to learn about typical behaviour of different learners across different datasets, and to construct surrogate models for tuning algorithm benchmarks. . AutoML Benchmark (Janek) . A study was created containing 76 binary and multiclass tasks of reasonable difficulty. . These can be used as a more difficult version of OpenML-100 or in amlb a platform for reproducible benchmarking of AutoML systems. . R API . Short session on how the R api will (need to) change. The main issue discussed was that the OpenML R package runs with mlr and breaks when the new package (mlr3) is loaded. We will update the current OpenML R package to work with mlr3. At the same time we are thinking about a vision for a rewrite of the OpenML R package. . Python API . We made a lot of improvements to the Python API over the week, with over 20 PRs merged! We’ve added more examples on how to use the package, fixed bugs, improved documentation and refactored code. In the coming days we’re going to make all these improvements available in a new PyPI release. For those looking for a higher level overview of the package, we will publish a paper next week which highlights use-cases, its software design, and project structure. . Benchmarking paper . We are working on a comprehensive paper using sklearn, mlr and WEKA, which should demonstrate how OpenML can be used for proper benchmarking and analysis. . Guidelines / Overfitting / Comparable Metalearning . There are plans for writing a guidelines and pitfalls paper on benchmarking, meta-overfitting and statistical analysis of results on OpenML. . Data Formats . Currently OpenML supports only tabular data in ARFF data format. This is very limiting for many ML tasks. We discussed and explored other data formats we could use as the future next data format. We will post a separate blog post about our process and insights. . Funding . OpenML is looking for funding (developers). New ideas on obtaining funding are very welcome. We discussed some ideas: American funding (we need a collaboration partner); ALICE / CLAIRE; Companies. We are a foundation now, which might make it easier. . Some of the PIs (in particular Bernd Bischl, Frank Hutter and Dawn Song) in the project offer positions with a mix of ML research and development. Contact them if you are interested! . . . We had some talks at the workshop as well: . Mitar Milutinovic: A short introduction to Data Driven Discovery (D3M) . | Yiwen Zhu and Markus Weimer: Large-scale analysis of Jupyter notebooks . | Martin Binder, Michel Lang, Florian Pfisterer, Bernd Bischl: Pipelining with mlr3 . | . … and lots of fun… . . Wanna join the OpenML community? Get in touch! .",
            "url": "https://openml.github.io/blog/openml/2019/10/24/OpenML-workshop-at-Dagstuhl.html",
            "relUrl": "/openml/2019/10/24/OpenML-workshop-at-Dagstuhl.html",
            "date": " • Oct 24, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Basic components of OpenML",
            "content": "During my PhD, we developed OpenML, an online experiment database for Machine Learning. Researchers are encouraged to upload their experimental results on it, so that these can be reused by anyone. Various high level papers have been published that overview the design goals, benefits and opportunities (for example, at ECML/PKDD 2013, SIGKDD Explorations and JLMR). However, there is no clear overview of the basic components upon which the platform is build. In this blog post I will review these, and discuss some best practises. . Data . One of the core components of OpenML are datasets. People can upload their datasets, and the system automatically organises these on line. An example of a dataset is the well-known Iris dataset. It shows all features, once of these is identified as the ‘default target attribute’, although this concept is flexible. It also shows some automatically computed data qualities (or, meta-features). Each dataset has it’s own unique ID. Information about the dataset, the data features and the data qualities can be obtained automatically by means of the following API functions: . Get all available datasets . | Get dataset (required the data id) . | Get data features (requires the data id) . | Get data qualities (requires the data id) . | . Task types and tasks . A dataset alone does not constitute a scientific task. We must first agree on what types of results are expected to be shared. This is expressed in task types: they define what types of inputs are given, which types of output are expected to be returned, and what protocols should be used. For instance, classification tasks should include well-defined cross-validation procedures, labelled input data, and require predictions as outputs. The collection of all this information together is called a task. The Iris dataset has various tasks defined on it, for example this one. Although the web-interface does not show it, this task formally describes the target attribute that should be modelled (in this case the same as the default target attribute of the dataset, but this is flexible), the quality estimation procedure (10-fold cross-validation), the evaluation measure (predictive accuracy) and the cross-validation folds. Useful API operations include: . Get all available tasks . | Get all available tasks of a given type (e.g. get all Classification tasks, requires the id of the task type) . | Get the details of a task (requires task id) . | . Currently, there are a wide range of task types defined on OpenML, including classification, regression, on line learning, clustering and subgroup discovery. Although this set can be extended, this is currently not a supported API operation (meaning that we will add them by hand). If you interested in task types that are currently not supported, please contact us. . Flows . Tasks can be ‘solved’ by classifiers (or algorithms, workflows, flows). OpenML stores references to these flows. It is important to stress that flows are actually ran on the computer of the user, only meta-information about the flow is stored on OpenML. This information includes basic trivialities such as the creator, toolbox and compilation instructions, but also more formal description about hyper parameter. A flow can also contain subflows, for example, the flow Bagging can have a subflow ‘Decision Tree’ which would make the flow ‘Bagging of Decision Trees’. A flow is distinguished by it’s name and ‘external version’, which are both provided by the uploader. When uploading a flow, it is important to think about a good naming convention for the both, for example, the git commit number could be used as external version, as this uniquely identifies a state of the code. Ideally, when two persons are using the same flow, they will use the same name and external version, so that results of the flows can be compared across tasks. (This is ensured when using the toolboxed in which OpenML is integrated, such as Weka, Scikit Learn and MLR). Useful API functions are: . List all flows . | List all my flows . | Give details about a given flow (requires flow id) . | . Runs . Whenever a flow executes a task, this is called a run. The existence of runs is actually the main contribution of OpenML. Some experiments take weeks to complete, and having the results stored on OpenML helps other researchers resuse the experiments. The task description specifies which information should be uploaded in order to have a valid run, in most cases, for each cross-validation fold the predictions on the test set. This allows OpenML to calculate basic evaluation measures, such as predictive accuracy, ROC curves and many more. Also information about the flow and hyper parameter settings should be provided. Some useful API functions: . List all runs performed on a given task (requires task id, e.g., the iris task is 59) . | Compare two flows on all tasks (requires a comma separated list of flow ids, e.g., 1720, 1721 for comparing k-nn with a decision tree) . | And many more … . | . Usually, the result is in some XML or JSON format (depending on the preference of the user), linking together various task ids, flow ids, etc. In order for this to become meaningful, the user needs to perform other API tasks to get information about what flows were executed, what tasks and datasets were used, etc. Details about this will be provided in another post. . Setups . Every run that is executed by a flow, contains information about the hyper parameter settings of the flow. A setup is the combination of all parameter settings of a given flow. OpenML internally links the result of a given run to a setup id. This way, experiments can be done across hyper parameter settings. For example, . Compare two setups on all tasks (requires a comma separated list of setup ids, e.g., 8994, 8995, 8996 for comparing multiple MLP configurations) | . As setups constitute a complex concept, most of the operations concerning setups are hidden from the user. Hence, not all setup functions are properly documented yet. A later blogpost will detail on these. .",
            "url": "https://openml.github.io/blog/openml/2017/03/03/Basic-components-of-OpenML.html",
            "relUrl": "/openml/2017/03/03/Basic-components-of-OpenML.html",
            "date": " • Mar 3, 2017"
        }
        
    
  
    
        ,"post3": {
            "title": "mlr loves OpenML",
            "content": "OpenML stands for Open Machine Learning and is an online platform, which aims at supporting collaborative machine learning online. It is an Open Science project that allows its users to share data, code and machine learning experiments. . At the time of writing this blog post I am in Eindhoven at an OpenML workshop, where developers and scientists meet to work on improving the project. Some of these people are R users and they (we) are developing an R package that communicates with the OpenML platform. . . OpenML in R . The OpenML R package can list and download data sets and machine learning tasks (prediction challenges). In R one can run algorithms on the these data sets/tasks and then upload the results to OpenML. After successful uploading, the website shows how well the algorithm performs. To run the algorithm on a given task the OpenML R package builds on the mlr package. mlr understands what a task is and can run learners on that task. So all the OpenML package needs to do is convert the OpenML objects to objects mlr understands and then mlr deals with the learning. . A small case study . We want to create a little study on the OpenML website, in which we compare different types of Support Vector Machines. The study gets an ID assigned to it, which in our case is 27. We use the function ksvm (with different settings of the function argument type) from package kernlab, which is integrated in mlr (“classif.ksvm”). . . For details on installing and setting up the OpenML R package please see the guide on GitHub. . Let’s start conducting the study: . Load the packages and list all tasks which have between 100 and 500 observations. | . library(&quot;OpenML&quot;) library(&quot;mlr&quot;) library(&quot;farff&quot;) library(&quot;BBmisc&quot;) dsize = c(100, 500) taskinfo_all = listOMLTasks(number.of.instances = dsize) . Select all supervised classification tasks that do 10-fold cross-validation and choose only one task per data set. To keep the study simple and fast to compute, select only the first three tasks. | . taskinfo_10cv = subset(taskinfo_all, task.type == &quot;Supervised Classification&quot; &amp; estimation.procedure == &quot;10-fold Crossvalidation&quot; &amp; evaluation.measures == &quot;predictive_accuracy&quot; &amp; number.of.missing.values == 0 &amp; number.of.classes %in% c(2, 4)) taskinfo = taskinfo_10cv[1:3, ] . Create the learners we want to compare. | . lrn.list = list( makeLearner(&quot;classif.ksvm&quot;, type = &quot;C-svc&quot;), makeLearner(&quot;classif.ksvm&quot;, type = &quot;kbb-svc&quot;), makeLearner(&quot;classif.ksvm&quot;, type = &quot;spoc-svc&quot;) ) . Run the learners on the three tasks. | . grid = expand.grid(task.id = taskinfo$task.id, lrn.ind = seq_along(lrn.list)) runs = lapply(seq_row(grid), function(i) { message(i) task = getOMLTask(grid$task.id[i]) ind = grid$lrn.ind[i] runTaskMlr(task, lrn.list[[ind]]) }) . And finally upload the runs to OpenML. The upload function (uploadOMLRun) returns the ID of the uploaded run object. When uploading runs that are part of a certain study, tag it with study_ and the study ID. After uploading the runs appear on the website and can be found using the tag or via the study homepage. | . ## please do not spam the OpenML server by uploading these ## tasks. I already did that. run.id = lapply(runs, uploadOMLRun, tags = &quot;study_27&quot;) . To show the results of our study, list the run evaluations and make a nice plot. | . evals = listOMLRunEvaluations(tag = &quot;study_27&quot;) evals$task.id = as.factor(evals$task.id) evals$setup.id = as.factor(evals$setup.id) library(&quot;ggplot2&quot;) ggplot(evals, aes(x = setup.id, y = predictive.accuracy, color = data.name, group = task.id)) + geom_point() + geom_line() . . Now you can go ahead and create a bigger study using the techniques you have learned. . Further infos . If you are interested in more, check out the OpenML blog, the paper and the GitHub repos. . . Originally published at mlr-org.github.io. .",
            "url": "https://openml.github.io/blog/openml/mlr/r/2016/09/18/mlr-loves-OpenML.html",
            "relUrl": "/openml/mlr/r/2016/09/18/mlr-loves-OpenML.html",
            "date": " • Sep 18, 2016"
        }
        
    
  
    
        ,"post4": {
            "title": "OpenML",
            "content": "OpenML is a very cool new online platform that aims at improving — as the name says — Open Machine Learning. It stands for Open Data, Open Algorithms and Open Research. OpenML is still in it’s beta phase, but already pretty awesome. . With this blog post I would like to introduce the main concepts, show who should be interested in the platform and I will go a little into a challenge it faces. . Concepts . The following four concepts form the basis of the platform: . data . | task . | flow . | run . | . The figure shows how they are connected. . . Who can make use of OpenML? . The domain scientist . You have data that you do not know how to analyse best? Upload your data to OpenML and you will have the whole world helping you. Write a good data and task description to make sure people understand the problem. . The data analyst . You like taking part in challenges? Being the best solver of a task? Go to OpenML and check out the many tasks and go solve! . The algorithm developer . You developed a statistical method or a machine learning algorithm and want to try it out? You will find plenty of data sets and the possibility to make your algorithm public. . The student . You study statistics, data science, machine learning? You want to know what is out there? On OpenML you will find a wide variety of algorithms and, if the solvers do a good job, info on software and implementation. . The teacher . You teach a machine learning class and want the students to participate in a challenge? Make up your own task and let the students try solving it. The platform shows who uploaded what and when. . The unknown . There are possibly many other people who will benefit from the platform, like meta analysts, benchmarkers and people I can not think of right now. . How to use OpenML . Other than just browsing the website you can access OpenML through quite some interfaces such as R or WEKA. For an example on how to use the R interface check out the tutorial. . The whole project is of course open source. Check out the different git repositories for all the code and in case you have any complaints. . The overfitting problem . Platforms like kaggle, crowdanalytics and innocentive host challenges and give people only part of the data so they can evaluate the performance of the algorithm on a separate data set to (try to) prevent overfitting. So far OpenML does not do that. It always shows all the data, and algorithms are evaluated via resampling procedures (on OpenML called estimation procedures). There are big discussions about how to solve the problem of overfitting on OpenML. They go from keeping part of the data hidden for a certain time in the beginning to doing repeated cross-validation on the (overly) good performing flows on a given task. If you have ideas here, please don’t hesitate to leave me a comment. . The platform is still in it’s childhood and may not be perfect yet (If you find issues, post them on the github page). But I think it can grow to be a great thing one day. . . Originally published at heidiseibold.github.io on May 2, 2016. .",
            "url": "https://openml.github.io/blog/openml/2016/05/02/OpeML.html",
            "relUrl": "/openml/2016/05/02/OpeML.html",
            "date": " • May 2, 2016"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About OpenML",
          "content": "OpenML is an open source platform where you can host, review, share, and organize machine learning datasets, algorithms, and experiments through clean and open interfaces. We are a group of machine learning researchers and enthousiasts who believe in making machine learning more streamlined, accessible, and beneficial to all of humanity. .",
          "url": "https://openml.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://openml.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}